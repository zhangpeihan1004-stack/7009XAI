import streamlit as st
import torch
import numpy as np
from PIL import Image
import matplotlib.pyplot as plt
from lime import lime_image
from skimage.segmentation import mark_boundaries
from transformers import BlipProcessor, BlipForQuestionAnswering
import torch.nn.functional as F

# --- 1. é¡µé¢åŸºç¡€é…ç½® ---
st.set_page_config(
    page_title="VQA-RAD åŒ»ç–—è¯Šæ–­ç³»ç»Ÿ",
    page_icon="ğŸ¥",
    layout="wide"
)

# è‡ªå®šä¹‰ CSS è®©ç•Œé¢æ›´åƒåŒ»ç–—è½¯ä»¶
st.markdown("""
<style>
    .reportview-container {
        background: #f0f2f6;
    }
    .main-header {
        font-size: 2.5rem;
        color: #0e76a8;
        text-align: center;
        margin-bottom: 1rem;
    }
    .diagnosis-box {
        background-color: #e8f4f8;
        padding: 20px;
        border-radius: 10px;
        border-left: 5px solid #0e76a8;
        margin-bottom: 20px;
    }
</style>
""", unsafe_allow_html=True)

st.markdown('<div class="main-header">ğŸ¥ æ™ºèƒ½åŒ»ç–—å½±åƒè¾…åŠ©è¯Šæ–­ç³»ç»Ÿ (BLIP-XAI)</div>', unsafe_allow_html=True)


# --- 2. åŠ è½½æ¨¡å‹ (æ ¸å¿ƒéƒ¨åˆ†) ---
@st.cache_resource
def load_model():
    """
    åŠ è½½æ¨¡å‹å¹¶ç¼“å­˜ï¼Œé¿å…æ¯æ¬¡åˆ·æ–°é¡µé¢éƒ½é‡æ–°ä¸‹è½½
    """
    device = "cuda" if torch.cuda.is_available() else "cpu"
    # ä½¿ç”¨ Salesforce çš„åŸºç¡€ BLIP æ¨¡å‹
    processor = BlipProcessor.from_pretrained("Salesforce/blip-vqa-base")
    model = BlipForQuestionAnswering.from_pretrained("Salesforce/blip-vqa-base").to(device)
    return processor, model, device


# æ˜¾ç¤ºåŠ è½½çŠ¶æ€
with st.spinner('æ­£åœ¨åˆå§‹åŒ–åŒ»ç–— AI å¼•æ“ (åŠ è½½ BLIP æ¨¡å‹)...'):
    processor, model, device = load_model()


# --- 3. æ ¸å¿ƒé¢„æµ‹ä¸è§£é‡Šå‡½æ•° ---

def predict_answer(image, question):
    """
    è·å– BLIP çš„æ–‡æœ¬å›ç­”
    """
    inputs = processor(image, question, return_tensors="pt").to(device)
    out = model.generate(**inputs)
    answer = processor.decode(out[0], skip_special_tokens=True)
    return answer


def lime_predict_proba(images, question, target_label_idx=None):
    """
    é€‚é… LIME çš„é¢„æµ‹å‡½æ•°ã€‚
    LIME éœ€è¦è¾“å…¥ numpy æ•°ç»„ï¼Œè¾“å‡ºæ¦‚ç‡ã€‚
    è¿™é‡Œæˆ‘ä»¬è®¡ç®—æ¨¡å‹ç”Ÿæˆç‰¹å®šç­”æ¡ˆçš„æ¦‚ç‡ã€‚
    """
    # LIME ä¼ å…¥çš„æ˜¯ numpy æ•°ç»„åˆ—è¡¨ï¼Œè½¬ä¸º PIL
    pil_images = [Image.fromarray(img.astype(np.uint8)) for img in images]

    # æ„é€  batch è¾“å…¥
    inputs = processor(images=pil_images, text=[question] * len(pil_images), return_tensors="pt", padding=True).to(
        device)

    # è·å– logits
    with torch.no_grad():
        outputs = model(**inputs)
        # è·å–è¯è¡¨ä¸­æ–‡æœ¬ logits
        logits = outputs.text_outputs.logits[:, 0, :]  # å–ç¬¬ä¸€ä¸ª token
        probs = F.softmax(logits, dim=-1)

    # å¦‚æœæ²¡æœ‰æŒ‡å®šç›®æ ‡ labelï¼Œå°±å–å½“å‰æœ€å¤§æ¦‚ç‡çš„ label ä½œä¸ºç›®æ ‡
    if target_label_idx is None:
        target_label_idx = torch.argmax(probs[0]).item()

    # LIME éœ€è¦è¿”å› (batch_size, num_classes)ï¼Œä¸ºäº†ç®€åŒ–ï¼Œæˆ‘ä»¬åªè¿”å›ç›®æ ‡ç±»çš„æ¦‚ç‡
    # æ„é€ ä¸€ä¸ªä¼ªæ¦‚ç‡ï¼š[ç›®æ ‡ç±»æ¦‚ç‡, 1-ç›®æ ‡ç±»æ¦‚ç‡]
    target_probs = probs[:, target_label_idx].cpu().numpy()
    return np.stack([1 - target_probs, target_probs], axis=1)


# --- 4. ä¾§è¾¹æ ï¼šæ§åˆ¶åŒº ---
st.sidebar.title("ğŸ©º è¯Šæ–­æ§åˆ¶å°")

uploaded_file = st.sidebar.file_uploader("1. ä¸Šä¼ å½±åƒ (X-Ray/CT)", type=["jpg", "png", "jpeg"])

# é¢„è®¾é—®é¢˜ï¼Œæ–¹ä¾¿æ¼”ç¤º
question_options = [
    "Is there a fracture?",
    "Is the lung normal?",
    "What is the abnormality?",
    "Is the heart enlarged?",
    "è‡ªå®šä¹‰é—®é¢˜..."
]
selected_q = st.sidebar.selectbox("2. é€‰æ‹©è¯Šæ–­é—®é¢˜", question_options)
if selected_q == "è‡ªå®šä¹‰é—®é¢˜...":
    question = st.sidebar.text_input("è¯·è¾“å…¥é—®é¢˜ (è‹±æ–‡)", "Is there a fracture?")
else:
    question = selected_q

# LIME è®¾ç½® (ç”¨äºå¹³è¡¡é€Ÿåº¦å’Œè´¨é‡)
st.sidebar.markdown("---")
st.sidebar.subheader("ğŸ”¬ XAI è®¾ç½®")
num_samples = st.sidebar.slider("LIME é‡‡æ ·æ•° (è¶Šé«˜è´¨é‡è¶Šå¥½ä½†è¶Šæ…¢)", 50, 500, 100)

# --- 5. ä¸»ç•Œé¢é€»è¾‘ ---

if uploaded_file is not None:
    # å¸ƒå±€ï¼šå·¦å›¾å³æ–‡
    col1, col2 = st.columns([1, 1.2])

    # åŠ è½½å¹¶æ˜¾ç¤ºåŸå›¾
    raw_image = Image.open(uploaded_file).convert('RGB')

    with col1:
        st.subheader("åŸå§‹å½±åƒ")
        st.image(raw_image, use_column_width=True, caption="Uploaded Patient Scan")

    # æŒ‰é’®è§¦å‘åˆ†æ
    if st.sidebar.button("å¼€å§‹ AI è¯Šæ–­ä¸åˆ†æ", type="primary"):
        with col2:
            st.subheader("è¯Šæ–­æŠ¥å‘Š")

            # æ­¥éª¤ A: é¢„æµ‹
            with st.spinner('ğŸ¤– AI æ­£åœ¨é˜…ç‰‡å¹¶ç”Ÿæˆè¯Šæ–­...'):
                diagnosis = predict_answer(raw_image, question)

            # æ˜¾ç¤ºæ¼‚äº®çš„è¯Šæ–­æ¡†
            st.markdown(f"""
            <div class="diagnosis-box">
                <h4><b>Q:</b> {question}</h4>
                <h3><b>A:</b> {diagnosis}</h3>
            </div>
            """, unsafe_allow_html=True)

            # æ­¥éª¤ B: XAI å¯è§†åŒ– (LIME)
            st.subheader("å¯è§£é‡Šæ€§åˆ†æ (LIME)")
            progress_bar = st.progress(0)
            status_text = st.empty()

            try:
                status_text.text("æ­£åœ¨åˆå§‹åŒ– LIME è§£é‡Šå™¨...")
                explainer = lime_image.LimeImageExplainer()

                # å®šä¹‰é’ˆå¯¹å½“å‰é—®é¢˜å’Œå›¾ç‰‡çš„é¢„æµ‹å‡½æ•° wrapper
                # æˆ‘ä»¬éœ€è¦æ‰¾åˆ° answer å¯¹åº”çš„ token ID
                inputs = processor(raw_image, question, return_tensors="pt").to(device)
                out = model.generate(**inputs)
                predicted_token_id = out[0][1]  # å–ç”Ÿæˆçš„ç¬¬ä¸€ä¸ªæœ‰æ•ˆ token (é€šå¸¸æ˜¯ [CLS] åçš„ç¬¬ä¸€ä¸ª)

                # åŒ…è£…å‡½æ•°
                predict_fn_lime = lambda x: lime_predict_proba(x, question, target_label_idx=predicted_token_id)

                status_text.text(f"æ­£åœ¨ç”Ÿæˆæ‰°åŠ¨æ ·æœ¬ (Samples: {num_samples})... è¿™å¯èƒ½éœ€è¦ä¸€åˆ†é’Ÿ")
                progress_bar.progress(30)

                # æ ¸å¿ƒ LIME è®¡ç®—
                explanation = explainer.explain_instance(
                    np.array(raw_image),
                    predict_fn_lime,
                    top_labels=1,
                    hide_color=0,
                    num_samples=num_samples
                )
                progress_bar.progress(80)

                # è·å–å›¾åƒå’Œæ©è†œ
                temp, mask = explanation.get_image_and_mask(
                    explanation.top_labels[0],
                    positive_only=True,
                    num_features=5,
                    hide_rest=False
                )

                # æ˜¾ç¤º LIME ç»“æœ
                fig, ax = plt.subplots()
                img_boundary = mark_boundaries(temp / 255.0 + 0.5, mask)  # ç¨å¾®è°ƒäº®ä¸€ç‚¹
                ax.imshow(img_boundary)
                ax.axis('off')
                ax.set_title(f"LIME Visualization for '{diagnosis}'")

                st.pyplot(fig)
                progress_bar.progress(100)
                status_text.text("âœ… åˆ†æå®Œæˆ")

                st.info(
                    f"**å›¾è§£è¯´æ˜ï¼š** é»„è‰²/é«˜äº®è¾¹ç¼˜åŒºåŸŸè¡¨ç¤º AI åœ¨åˆ¤å®š '{diagnosis}' æ—¶é‡ç‚¹å…³æ³¨çš„å›¾åƒç‰¹å¾ (Superpixels)ã€‚")

                # ç”Ÿæˆå¯ä¸‹è½½æŠ¥å‘Š
                report_content = f"""
                === VQA-RAD DIAGNOSTIC REPORT ===
                Image: {uploaded_file.name}
                Clinical Question: {question}
                AI Diagnosis: {diagnosis}
                XAI Method: LIME (Local Interpretable Model-agnostic Explanations)
                Confidence Areas: Identified in the attached visualization.
                =================================
                """
                st.download_button("ğŸ“¥ ä¸‹è½½å®Œæ•´è¯Šæ–­æŠ¥å‘Š", report_content, "diagnosis_report.txt")

            except Exception as e:
                st.error(f"XAI ç”Ÿæˆè¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(e)}")
                st.write("å»ºè®®ï¼šå°è¯•å‡å°‘ LIME é‡‡æ ·æ•°æˆ–æ£€æŸ¥æ˜¾å­˜ã€‚")

else:
    # æ¬¢è¿é¡µçŠ¶æ€
    st.info("ğŸ‘ˆ è¯·åœ¨å·¦ä¾§ä¾§è¾¹æ ä¸Šä¼ ä¸€å¼ åŒ»å­¦å½±åƒä»¥å¼€å§‹æ¼”ç¤ºã€‚")

    # æ¼”ç¤ºç”¨çš„ä¼ªä»£ç å±•ç¤º (å¯é€‰)
    with st.expander("æŸ¥çœ‹ Dashboard åŸç† (ä»£ç ç‰‡æ®µ)"):
        st.code("""
        # æ ¸å¿ƒé€»è¾‘
        diagnosis = model.generate(image, question)
        explanation = lime.explain_instance(image, predict_fn)
        st.pyplot(explanation.show())
        """, language='python')